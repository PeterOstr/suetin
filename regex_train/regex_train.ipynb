{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-02T08:34:40.455674Z",
     "start_time": "2024-03-02T08:34:40.428676Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode, split, regexp_replace\n",
    "from pyspark.sql.functions import col, split, explode, regexp_replace\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import lit, col, when\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.ml.feature import FeatureHasher\n",
    "import os\n",
    "from pyspark.sql.functions import col, when, lit, isnan\n",
    "from pyspark.sql.functions import lit, nanvl, isnan, when, avg\n",
    "import category_encoders as ce\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from category_encoders import CatBoostEncoder\n",
    "import pickle\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import DoubleType, ArrayType\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "sklearn.set_config(transform_output='pandas')\n",
    "from pyspark.sql.functions import array\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession, types as T\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import round, sum, avg, count, min, max\n",
    "from ast import literal_eval\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "from pyspark.sql import functions as sf\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "# from geospark.register import upload_jars\n",
    "# from geospark.register import GeoSparkRegistrator\n",
    "# from geospark.core.SpatialRDD import PolygonRDD, PointRDD\n",
    "# from geospark.core.enums import FileDataSplitter\n",
    "# from geospark.core.enums import IndexType\n",
    "# from geospark.core.geom.envelope import Envelope\n",
    "# from geospark.core.spatialOperator import RangeQuery\n",
    "# from geospark.core.formatMapper.shapefileParser import ShapefileReader\n",
    "# from geospark.sql.types import GeometryType\n",
    "# from geospark.register import sparkJVM\n",
    "# import re\n",
    "# upload_jars()\n",
    "#\n",
    "#\n",
    "#\n",
    "# spark = SparkSession.builder.appName(\"Spark_v1\")    .config(\"spark.default.parallelism\", 176*3) \\\n",
    "#     .config(\"spark.sql.shuffle.partitions\", 176*3) \\\n",
    "#     .config(\"spark.executor.memory\", \"50g\") \\\n",
    "#     .config(\"spark.executor.cores\", \"5\") \\\n",
    "#     .config(\"spark.driver.memory\", \"10g\") \\\n",
    "#     .config(\"spark.driver.maxResultSize\", \"1300g\") \\\n",
    "#     .getOrCreate()\n",
    "import os\n",
    "\n",
    "# setup arguments\n",
    "#os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0'\n",
    "\n",
    "# initialize spark\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, BooleanType\n",
    "\n",
    "\n",
    "ram = 16\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    appName('spark_first_run') \\\n",
    "    .config(\"spark.executor.memory\", f\"{ram}g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", f\"{ram}g\") \\\n",
    "    .config(\"spark.driver.memory\", f\"{ram}g\") \\\n",
    "    .config(\"spark.executor.memoryOverhead\", f\"{ram}g\") \\\n",
    "    .config(\"spark.sql.debug.maxToStringFields\", \"100000\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "#    .config(\"spark.driver.memory\", \"10g\") \\\n",
    "\n",
    "#    .config(\"spark.executor.memoryOverhead\", \"10g\") \\\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Register Sedona functions to Spark\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import regexp_extract\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from scipy.stats import t\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import udf, lit\n",
    "\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "import json\n",
    "from pyspark.sql.functions import udf\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import udf, explode\n",
    "from pyspark.sql.types import ArrayType, StructType, StructField, FloatType\n",
    "from pyspark.sql.functions import desc\n",
    "from time import sleep\n",
    "from pyspark.sql import Row\n",
    "#from pyspark.sql.functions import min, max\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import split, explode, lower, trim\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when, count, col\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import row_number\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.functions import monotonically_increasing_id, concat, col, lit, cast\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import collect_list, udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import initcap\n",
    "import re\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# This is regex train notebook"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c582e741e0005a05"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Uploading data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e104547328a17367"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         Link\n",
      "0                      https://www.scribd.com\n",
      "1               https://www.scrdownloader.com\n",
      "2  http://bemowsap08v.corp.kpmgconsulting.com\n",
      "3                        https://help.sap.com\n",
      "4                        https://help.sap.com\n"
     ]
    }
   ],
   "source": [
    "# Function to read a text file and find web links in it using regular expressions\n",
    "def find_links_in_text_file(file_path):\n",
    "    # Open the text file for reading\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Pattern for finding web links\n",
    "    pattern = r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'\n",
    "\n",
    "    # Find all web links in the text using regular expression\n",
    "    links = re.findall(pattern, text)\n",
    "\n",
    "    # Create a Pandas DataFrame to store the found links\n",
    "    df = pd.DataFrame(links, columns=['Link'])\n",
    "\n",
    "    return df\n",
    "\n",
    "# Path to the text file\n",
    "file_path = 'C:/Users/Peter/DataspellProjects/suetin/regex_train/data/_chat.txt'\n",
    "\n",
    "# Get a DataFrame with the found web links\n",
    "links_df = find_links_in_text_file(file_path)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(links_df.head())\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T09:20:03.405828Z",
     "start_time": "2024-03-02T09:20:03.347462Z"
    }
   },
   "id": "499c87f0d91ceee",
   "execution_count": 60
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                                         Ссылка\n0                        https://www.scribd.com\n1                 https://www.scrdownloader.com\n2    http://bemowsap08v.corp.kpmgconsulting.com\n3                          https://help.sap.com\n4                          https://help.sap.com\n..                                          ...\n548                   https://chrome.google.com\n549                       https://blogs.sap.com\n550                    https://pmllc.omeclk.com\n551                       https://blogs.sap.com\n552   https://bearingpointrus-my.sharepoint.com\n\n[553 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Ссылка</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>https://www.scribd.com</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>https://www.scrdownloader.com</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>http://bemowsap08v.corp.kpmgconsulting.com</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>https://help.sap.com</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>https://help.sap.com</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>548</th>\n      <td>https://chrome.google.com</td>\n    </tr>\n    <tr>\n      <th>549</th>\n      <td>https://blogs.sap.com</td>\n    </tr>\n    <tr>\n      <th>550</th>\n      <td>https://pmllc.omeclk.com</td>\n    </tr>\n    <tr>\n      <th>551</th>\n      <td>https://blogs.sap.com</td>\n    </tr>\n    <tr>\n      <th>552</th>\n      <td>https://bearingpointrus-my.sharepoint.com</td>\n    </tr>\n  </tbody>\n</table>\n<p>553 rows × 1 columns</p>\n</div>"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T08:34:41.378035Z",
     "start_time": "2024-03-02T08:34:41.354036Z"
    }
   },
   "id": "2967c2eb43720a28",
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                                           link\n0                        https://www.scribd.com\n1                 https://www.scrdownloader.com\n2    http://bemowsap08v.corp.kpmgconsulting.com\n3                          https://help.sap.com\n4                          https://help.sap.com\n..                                          ...\n548                   https://chrome.google.com\n549                       https://blogs.sap.com\n550                    https://pmllc.omeclk.com\n551                       https://blogs.sap.com\n552   https://bearingpointrus-my.sharepoint.com\n\n[553 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>link</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>https://www.scribd.com</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>https://www.scrdownloader.com</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>http://bemowsap08v.corp.kpmgconsulting.com</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>https://help.sap.com</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>https://help.sap.com</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>548</th>\n      <td>https://chrome.google.com</td>\n    </tr>\n    <tr>\n      <th>549</th>\n      <td>https://blogs.sap.com</td>\n    </tr>\n    <tr>\n      <th>550</th>\n      <td>https://pmllc.omeclk.com</td>\n    </tr>\n    <tr>\n      <th>551</th>\n      <td>https://blogs.sap.com</td>\n    </tr>\n    <tr>\n      <th>552</th>\n      <td>https://bearingpointrus-my.sharepoint.com</td>\n    </tr>\n  </tbody>\n</table>\n<p>553 rows × 1 columns</p>\n</div>"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links_df = links_df.rename(columns={'Ссылка':'link'})\n",
    "links_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T08:45:29.360556Z",
     "start_time": "2024-03-02T08:45:29.347557Z"
    }
   },
   "id": "b3390d0369c5a88c",
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "0                          https://www.scribd.com\n1                   https://www.scrdownloader.com\n2      http://bemowsap08v.corp.kpmgconsulting.com\n3                            https://help.sap.com\n4                            https://help.sap.com\n                          ...                    \n548                     https://chrome.google.com\n549                         https://blogs.sap.com\n550                      https://pmllc.omeclk.com\n551                         https://blogs.sap.com\n552     https://bearingpointrus-my.sharepoint.com\nName: link, Length: 553, dtype: object"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links_df['link']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T08:45:30.972110Z",
     "start_time": "2024-03-02T08:45:30.952897Z"
    }
   },
   "id": "f25270bd7f1bad60",
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1 Finding all site names in links"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "19a5e26796b0a87"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "0             scribd\n1      scrdownloader\n2               None\n3                sap\n4                sap\n           ...      \n548           chrome\n549            blogs\n550            pmllc\n551            blogs\n552             None\nLength: 553, dtype: object"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefixes = ['www','help','chat']\n",
    "pattern = fr'https://({\"|\".join(prefixes)}|)\\.?([a-zA-Z]+)(\\.[a-zA-Z]*)'\n",
    "def find_pattern(text):\n",
    "    result = re.search(pattern, text)\n",
    "    if result:\n",
    "        return result.group(2)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "links_df_new = links_df.apply(lambda x: find_pattern(x['link']), axis=1)\n",
    "links_df_new"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T08:52:33.096627Z",
     "start_time": "2024-03-02T08:52:33.075587Z"
    }
   },
   "id": "7966372d501f1d1a",
   "execution_count": 43
  },
  {
   "cell_type": "markdown",
   "source": [
    "### finding all unique names"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da6f8f1cc2a6e4b6"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "              site\n0           scribd\n1    scrdownloader\n2             None\n3              sap\n5    marinetraffic\n..             ...\n524    cyclingrace\n528            api\n543             cp\n548         chrome\n550          pmllc\n\n[146 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>site</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>scribd</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>scrdownloader</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>sap</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>marinetraffic</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>524</th>\n      <td>cyclingrace</td>\n    </tr>\n    <tr>\n      <th>528</th>\n      <td>api</td>\n    </tr>\n    <tr>\n      <th>543</th>\n      <td>cp</td>\n    </tr>\n    <tr>\n      <th>548</th>\n      <td>chrome</td>\n    </tr>\n    <tr>\n      <th>550</th>\n      <td>pmllc</td>\n    </tr>\n  </tbody>\n</table>\n<p>146 rows × 1 columns</p>\n</div>"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links_df_new.name = 'site'\n",
    "links_df_new = pd.DataFrame(links_df_new)\n",
    "links_df_new['site'] = links_df_new['site'].replace('None', None)\n",
    "links_df_new['site'] = links_df_new['site'].drop_duplicates()\n",
    "\n",
    "links_df_new = links_df_new[links_df_new['site'] != None]  \n",
    "\n",
    "links_df_new"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T09:11:46.243800Z",
     "start_time": "2024-03-02T09:11:46.232338Z"
    }
   },
   "id": "f928212020a15055",
   "execution_count": 59
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'www|help|chat'"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'|'.join(prefixes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T08:34:42.408121Z",
     "start_time": "2024-03-02T08:34:42.392117Z"
    }
   },
   "id": "3df1b71037c03b2c",
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 Finding all site names in links without \"sap\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8c660df0522d8360"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "0                         https://www.scribd.com\n1                  https://www.scrdownloader.com\n2         http://bemowsap08v.corp.kpmgconsulting\n3                                           None\n4                                           None\n                         ...                    \n548                    https://chrome.google.com\n549                                         None\n550                     https://pmllc.omeclk.com\n551                                         None\n552    https://bearingpointrus-my.sharepoint.com\nLength: 553, dtype: object"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pattern = fr'https://(?!sap)\\.?([a-zA-Z]+)(\\.[a-zA-Z]*)'\n",
    "pattern = fr'https?://(?:www\\.)?(?!.*\\.sap)[a-zA-Z0-9-]+\\.[a-zA-Z]+(?:\\.[a-zA-Z]+)?'\n",
    "def find_pattern(text):\n",
    "    result = re.search(pattern, text)\n",
    "    if result:\n",
    "        return result.group(0)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "links_df_not_sap = links_df.apply(lambda x: find_pattern(x['Link']), axis=1)\n",
    "links_df_not_sap"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T09:54:02.902737Z",
     "start_time": "2024-03-02T09:54:02.877483Z"
    }
   },
   "id": "6fd4aec1da22c416",
   "execution_count": 70
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "pattern = r'www\\.(?!sap)'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9614f1a357faa089"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                                           link\n0                        https://www.scribd.com\n1                 https://www.scrdownloader.com\n2    http://bemowsap08v.corp.kpmgconsulting.com\n3                          https://help.sap.com\n4                          https://help.sap.com\n..                                          ...\n548                   https://chrome.google.com\n549                       https://blogs.sap.com\n550                    https://pmllc.omeclk.com\n551                       https://blogs.sap.com\n552   https://bearingpointrus-my.sharepoint.com\n\n[553 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>link</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>https://www.scribd.com</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>https://www.scrdownloader.com</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>http://bemowsap08v.corp.kpmgconsulting.com</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>https://help.sap.com</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>https://help.sap.com</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>548</th>\n      <td>https://chrome.google.com</td>\n    </tr>\n    <tr>\n      <th>549</th>\n      <td>https://blogs.sap.com</td>\n    </tr>\n    <tr>\n      <th>550</th>\n      <td>https://pmllc.omeclk.com</td>\n    </tr>\n    <tr>\n      <th>551</th>\n      <td>https://blogs.sap.com</td>\n    </tr>\n    <tr>\n      <th>552</th>\n      <td>https://bearingpointrus-my.sharepoint.com</td>\n    </tr>\n  </tbody>\n</table>\n<p>553 rows × 1 columns</p>\n</div>"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sap_links = links_df[links_df['Ссылка'].str.contains(r'sap')]\n",
    "# chosen_links = links_df[links_df['link'].str.contains(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+' )]\n",
    "chosen_links = links_df[links_df['link'].str.contains(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+' )]\n",
    "\n",
    "chosen_links"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T08:34:50.690570Z",
     "start_time": "2024-03-02T08:34:50.662557Z"
    }
   },
   "id": "453fe1ed1600f4b6",
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- link: string (nullable = true)\n",
      "+--------------------+\n",
      "|                link|\n",
      "+--------------------+\n",
      "|https://www.scrib...|\n",
      "|https://www.scrdo...|\n",
      "|http://bemowsap08...|\n",
      "|https://help.sap.com|\n",
      "|https://help.sap.com|\n",
      "|https://www.marin...|\n",
      "|http://bemowsap08...|\n",
      "|https://www.reart...|\n",
      "|https://www.tandf...|\n",
      "|http://bemowsap08...|\n",
      "|https://blogs.sap...|\n",
      "|https://blogs.sap...|\n",
      "|https://answers.s...|\n",
      "|https://answers.s...|\n",
      "|https://answers.s...|\n",
      "|        https://t.me|\n",
      "|https://cloud.bea...|\n",
      "|  https://medium.com|\n",
      "|  https://sapland.ru|\n",
      "|https://cloud.bea...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# df = spark.read.option(\"header\",True).txt('data/_chat.txt')\n",
    "df=spark.createDataFrame(links_df)\n",
    "df.printSchema()\n",
    "df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-01T20:03:22.955760Z",
     "start_time": "2024-03-01T20:03:21.772532Z"
    }
   },
   "id": "883396d53f10f941",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\\\\\(\\\\\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-01T20:03:25.409403Z",
     "start_time": "2024-03-01T20:03:25.397403Z"
    }
   },
   "id": "e06eeeba7db55e63",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                                           link\n0                        https://www.scribd.com\n1                 https://www.scrdownloader.com\n2    http://bemowsap08v.corp.kpmgconsulting.com\n3                          https://help.sap.com\n4                          https://help.sap.com\n..                                          ...\n548                   https://chrome.google.com\n549                       https://blogs.sap.com\n550                    https://pmllc.omeclk.com\n551                       https://blogs.sap.com\n552   https://bearingpointrus-my.sharepoint.com\n\n[553 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>link</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>https://www.scribd.com</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>https://www.scrdownloader.com</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>http://bemowsap08v.corp.kpmgconsulting.com</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>https://help.sap.com</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>https://help.sap.com</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>548</th>\n      <td>https://chrome.google.com</td>\n    </tr>\n    <tr>\n      <th>549</th>\n      <td>https://blogs.sap.com</td>\n    </tr>\n    <tr>\n      <th>550</th>\n      <td>https://pmllc.omeclk.com</td>\n    </tr>\n    <tr>\n      <th>551</th>\n      <td>https://blogs.sap.com</td>\n    </tr>\n    <tr>\n      <th>552</th>\n      <td>https://bearingpointrus-my.sharepoint.com</td>\n    </tr>\n  </tbody>\n</table>\n<p>553 rows × 1 columns</p>\n</div>"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T08:36:37.861881Z",
     "start_time": "2024-03-02T08:36:37.842778Z"
    }
   },
   "id": "d1b0979890dd27ca",
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "pattern = r'www\\.(?!sap)'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "81448b848eb58d6"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o219.saveAsTable.\n: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.validateTableLocation(SessionCatalog.scala:413)\r\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:176)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\r\n\tat org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:700)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:678)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:571)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[37], line 12\u001B[0m\n\u001B[0;32m      5\u001B[0m result \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39msql(\u001B[38;5;124m'''\u001B[39m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;124m    SELECT *\u001B[39m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;124m    FROM LINKS_DF\u001B[39m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;124m    WHERE NOT link LIKE \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124map\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;124m'''\u001B[39m)\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m# Отображаем результаты\u001B[39;00m\n\u001B[1;32m---> 12\u001B[0m \u001B[43mresult\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43moverwrite\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msaveAsTable\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mNEW_TABLE_WO_SAP\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     13\u001B[0m result\u001B[38;5;241m.\u001B[39mshow()\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\kanagawa\\lib\\site-packages\\pyspark\\sql\\readwriter.py:1586\u001B[0m, in \u001B[0;36mDataFrameWriter.saveAsTable\u001B[1;34m(self, name, format, mode, partitionBy, **options)\u001B[0m\n\u001B[0;32m   1584\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mformat\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   1585\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mformat\u001B[39m)\n\u001B[1;32m-> 1586\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msaveAsTable\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\kanagawa\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m   1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m-> 1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1323\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[0;32m   1326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\kanagawa\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[1;34m(*a, **kw)\u001B[0m\n\u001B[0;32m    177\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[0;32m    178\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 179\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m f(\u001B[38;5;241m*\u001B[39ma, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw)\n\u001B[0;32m    180\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    181\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\kanagawa\\lib\\site-packages\\py4j\\protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[1;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[0;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[0;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[1;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[0;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[0;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[0;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[0;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[0;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
      "\u001B[1;31mPy4JJavaError\u001B[0m: An error occurred while calling o219.saveAsTable.\n: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.validateTableLocation(SessionCatalog.scala:413)\r\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:176)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\r\n\tat org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:700)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:678)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:571)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n"
     ]
    }
   ],
   "source": [
    "# Регистрируем DataFrame как временную таблицу\n",
    "df.createOrReplaceTempView(\"LINKS_DF\")\n",
    "\n",
    "# Выполняем запрос SQL\n",
    "result = spark.sql('''\n",
    "    SELECT *\n",
    "    FROM LINKS_DF\n",
    "    WHERE NOT link LIKE '%sap%'\n",
    "''')\n",
    "\n",
    "# Отображаем результаты\n",
    "result.write.mode(\"overwrite\").saveAsTable(\"NEW_TABLE_WO_SAP\")\n",
    "result.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T08:36:43.331979Z",
     "start_time": "2024-03-02T08:36:43.229601Z"
    }
   },
   "id": "90c4740c80a4c290",
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "db75eb346176406b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2b6f6a6c5d0866b3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
